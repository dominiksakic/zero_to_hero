{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMps5N2oMvN/pAxcchbZF78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/zero_to_hero/blob/main/adv_05_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "Train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NXg2NidaWDY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## research and refresher\n",
        "\n",
        "- Simple example:\n",
        "  - 1 + 9 = 01\n",
        "  - then tokenize it:     1, 10, 9, 11, 0 , 1\n",
        "  - then create targets: -1, -1, -1, -1, 0, 1\n",
        "  - using ignore_index -1 will not calculate the crossentropy for  the index that match -1 in the targets.\n",
        "\n",
        "\n",
        "### Reminder of cross entropy:\n",
        "- [2.0, 1.0, 0.1]\n",
        "- Target 2\n",
        "\n",
        "- exp(2.0) = 7.3898\n",
        "- exp(1.0) = 2.7128\n",
        "- exp(0.1) = 1.105\n",
        "\n",
        "- sum = 10.212\n",
        "\n",
        "- 7.3898/10 = 0.7235\n",
        "- 2.7128/10 = 0.206\n",
        "- 1.105/10 = 0.108\n",
        "\n",
        "- target is 2 -> 0.108\n",
        "- -log(0.108) => 2.2256\n",
        "\n",
        "\n",
        "## Think about what are the targets of the inputs?\n",
        "- For gpt each token and the rest of the block size will be used to predict the next token."
      ],
      "metadata": {
        "id": "H2hjDA_gqHbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideation\n",
        "I have read about MoE that routes between different experts. I also have read about in GPT that there is a hard coded router or a router network that learns to which expert(Transformer) to send the input to.\n",
        "\n",
        "Could I create two 4 different experts, place the router NN in front of them and then build calculator like this?\n",
        "\n"
      ],
      "metadata": {
        "id": "nba9po1TFOzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '=']\n",
        "stoi = {ch:i for i, ch in enumerate(itos)}\n",
        "vocab_size = len(itos)\n",
        "\n",
        "def encode(s):\n",
        "  return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(tokens):\n",
        "  return ''.join([itos[t] for t in tokens])\n",
        "\n",
        "print(encode(\"1+9=01\"))  # [1, 10, 9, 11, 0, 1]\n",
        "print(decode([1, 10, 9, 11, 0, 1]))  # '1+9=01'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP0fGr8xviPG",
        "outputId": "5cb0e4ad-da90-4ba9-b7a3-c8170157c156"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10, 9, 11, 0, 1]\n",
            "1+9=01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def make_problem(max_digits: int = 2, reverse_answer: bool = True):\n",
        "    a = random.randint(0, 10**max_digits - 1)\n",
        "    b = random.randint(0, 10**max_digits - 1)\n",
        "    c = a + b\n",
        "    ans = str(c)[::-1] if reverse_answer else str(c)\n",
        "\n",
        "    x = encode(f\"{a}+{b}={ans}\")\n",
        "    y = x[1:] + [-1]\n",
        "\n",
        "    eq_id = stoi['=']\n",
        "    eq_pos = x.index(eq_id)\n",
        "    for t in range(eq_pos):\n",
        "        y[t] = -1\n",
        "\n",
        "    return x, y\n",
        "# test:\n",
        "x, y = make_problem()\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"decoded x:\", decode(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ1rYv-VL24Q",
        "outputId": "9564b0ae-ebb3-4ed7-bd5b-bf889283f807"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [1, 3, 10, 5, 1, 11, 4, 6]\n",
            "y: [-1, -1, -1, -1, -1, 4, 6, -1]\n",
            "decoded x: 13+51=46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(\n",
        "      batch_size: int,\n",
        "      max_digits : int =2,\n",
        "      reverse_answer : bool =True\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    xs, ys = [], []\n",
        "    for _ in range(batch_size):\n",
        "        x, y = make_problem(max_digits, reverse_answer)\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    max_len = max(len(seq) for seq in xs)\n",
        "    for i in range(batch_size):\n",
        "        pad_len = max_len - len(xs[i])\n",
        "        xs[i] = xs[i] + [0] * pad_len\n",
        "        ys[i] = ys[i] + [-1] * pad_len\n",
        "\n",
        "    xs = torch.tensor(xs)\n",
        "    ys = torch.tensor(ys)\n",
        "    xs, ys = xs.to(device), ys.to(device)\n",
        "    return xs, ys\n",
        "\n",
        "# test:\n",
        "xb, yb = make_batch(4)\n",
        "print(xb)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZfvZbSENpQT",
        "outputId": "1ebecfcf-1196-4fec-b99c-a056003d54f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 7,  6, 10,  5,  0, 11,  6,  2,  1],\n",
            "        [ 8,  6, 10,  4,  6, 11,  2,  3,  1],\n",
            "        [ 2,  9, 10,  8,  3, 11,  2,  1,  1],\n",
            "        [ 2,  2, 10,  7,  0, 11,  2,  9,  0]], device='cuda:0')\n",
            "tensor([[-1, -1, -1, -1, -1,  6,  2,  1, -1],\n",
            "        [-1, -1, -1, -1, -1,  2,  3,  1, -1],\n",
            "        [-1, -1, -1, -1, -1,  2,  1,  1, -1],\n",
            "        [-1, -1, -1, -1, -1,  2,  9, -1, -1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "max_digits = 3\n",
        "batch_size = 256\n",
        "block_size = 3 * max_digits + 3\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "cEHwKbKqUBif"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLAzSj-UFeN",
        "outputId": "7a5ed52c-dae7-4b1c-f2d2-7230904dcb5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e4124e6fc70>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rope(q, k):\n",
        "    # q, k: (B, n_head, T, head_size), where head_size must be even\n",
        "    B, nh, T, hs = q.shape\n",
        "    assert hs % 2 == 0, \"head_size must be even for RoPE\"\n",
        "    half = hs // 2\n",
        "\n",
        "    freqs = torch.exp(-torch.arange(0, half, dtype=torch.float32) * math.log(10000) / half).to(q.device)  # (half,)\n",
        "    positions = torch.arange(T, device=q.device).float()  # (T,)\n",
        "    angles = torch.einsum('t,d->td', positions, freqs)  # (T, half)\n",
        "    sin = angles.sin().unsqueeze(0).unsqueeze(0)  # (1, 1, T, half)\n",
        "    cos = angles.cos().unsqueeze(0).unsqueeze(0)  # (1, 1, T, half)\n",
        "\n",
        "    q1, q2 = q[..., :half], q[..., half:]\n",
        "    k1, k2 = k[..., :half], k[..., half:]\n",
        "    q_rotated = torch.cat([q1 * cos - q2 * sin, q1 * sin + q2 * cos], dim=-1)\n",
        "    k_rotated = torch.cat([k1 * cos - k2 * sin, k1 * sin + k2 * cos], dim=-1)\n",
        "    return q_rotated, k_rotated"
      ],
      "metadata": {
        "id": "9JovbhvNNukl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(eval_iters):\n",
        "        X, Y = make_batch(batch_size, max_digits=2, reverse_answer=True)\n",
        "        _, l = model(X, Y)\n",
        "        losses.append(l.item())\n",
        "    model.train()\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "3W5uLu3nUHC9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % num_heads == 0\n",
        "        self.n_head = num_heads\n",
        "        self.head_size = n_embd // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Linear projections for q, k, v (combined head projection)\n",
        "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "        # Final projection layer\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # (batch, time, channels)\n",
        "\n",
        "        # Project and reshape into multiple heads\n",
        "        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # Apply Rotary Positional Embedding if available\n",
        "        q, k = apply_rope(q, k)\n",
        "\n",
        "        # Compute attention weights\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, nh, T, T)\n",
        "\n",
        "        # Mask to prevent attending to future tokens (causal attention)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1, 1, T, T)\n",
        "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = wei @ v  # (B, nh, T, hs)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # reassemble heads (B, T, C)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "-FUXyH0JUq3z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_embd, n_head, dropout)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "FTqIhBDvU1CW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModelRoPE(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = self.blocks(tok_emb) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModelRoPE()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgZeKTTOVSC5",
        "outputId": "5b76aebc-86a1-4e74-ee87-22d10e119da5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.794892 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        loss = estimate_loss()\n",
        "        print(f\"step {iter}: random generated validation loss {loss:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = make_batch(batch_size=batch_size, max_digits=max_digits)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQdiY7OjVTP5",
        "outputId": "8c0d26bc-51bd-4c4a-8f73-765f10802498"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: random generated validation loss 0.1766\n",
            "step 100: random generated validation loss 0.2187\n",
            "step 200: random generated validation loss 0.1455\n",
            "step 300: random generated validation loss 0.3479\n",
            "step 400: random generated validation loss 0.1030\n",
            "step 500: random generated validation loss 0.0926\n",
            "step 600: random generated validation loss 0.1315\n",
            "step 700: random generated validation loss 0.0822\n",
            "step 800: random generated validation loss 0.0807\n",
            "step 900: random generated validation loss 0.0809\n",
            "step 1000: random generated validation loss 0.8111\n",
            "step 1100: random generated validation loss 0.1244\n",
            "step 1200: random generated validation loss 0.1117\n",
            "step 1300: random generated validation loss 0.0540\n",
            "step 1400: random generated validation loss 0.0727\n",
            "step 1500: random generated validation loss 0.0410\n",
            "step 1600: random generated validation loss 0.2927\n",
            "step 1700: random generated validation loss 0.0787\n",
            "step 1800: random generated validation loss 0.0355\n",
            "step 1900: random generated validation loss 0.0323\n",
            "step 2000: random generated validation loss 0.0354\n",
            "step 2100: random generated validation loss 0.3334\n",
            "step 2200: random generated validation loss 0.1149\n",
            "step 2300: random generated validation loss 0.0658\n",
            "step 2400: random generated validation loss 0.0523\n",
            "step 2500: random generated validation loss 0.0506\n",
            "step 2600: random generated validation loss 0.0238\n",
            "step 2700: random generated validation loss 0.0304\n",
            "step 2800: random generated validation loss 0.0189\n",
            "step 2900: random generated validation loss 0.0274\n",
            "step 3000: random generated validation loss 0.0373\n",
            "step 3100: random generated validation loss 0.1359\n",
            "step 3200: random generated validation loss 0.1083\n",
            "step 3300: random generated validation loss 0.0226\n",
            "step 3400: random generated validation loss 0.0203\n",
            "step 3500: random generated validation loss 0.0157\n",
            "step 3600: random generated validation loss 0.0220\n",
            "step 3700: random generated validation loss 0.0161\n",
            "step 3800: random generated validation loss 0.0115\n",
            "step 3900: random generated validation loss 0.0127\n",
            "step 4000: random generated validation loss 0.0842\n",
            "step 4100: random generated validation loss 0.1155\n",
            "step 4200: random generated validation loss 0.0400\n",
            "step 4300: random generated validation loss 0.0234\n",
            "step 4400: random generated validation loss 0.0160\n",
            "step 4500: random generated validation loss 0.0172\n",
            "step 4600: random generated validation loss 0.0140\n",
            "step 4700: random generated validation loss 0.0123\n",
            "step 4800: random generated validation loss 0.0397\n",
            "step 4900: random generated validation loss 0.2159\n",
            "step 4999: random generated validation loss 0.0517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "inp = torch.tensor([encode(\"226+336=\")], dtype=torch.long, device=device)\n",
        "out = model.generate(inp, max_new_tokens=3)\n",
        "print(\"decoded:\", decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLoq4hwGVV-j",
        "outputId": "51d88cb6-59b0-4e84-c260-19620b27d005"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoded: 226+336=265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem with masking and why my very try failed:\n",
        "\n",
        "```\n",
        "\"7+3=10\" (reversed answer \"01\").\n",
        "x = [7, '+', 3, '=', 1, 0]\n",
        "y = [-1, -1, -1, -1, 0, 1]\n",
        "```\n",
        "\n",
        "- the problem and the answer were alligned directly.\n",
        "\n",
        "- What I did was mask the tokens but not shift them!\n",
        "\n",
        "- the problem was that I was thinking of how I used to prepare the data like in tinyshakespear.\n",
        "  - I predicted the next character at each step, shifting it implicitly.\n",
        "- In addition each problem is short and discreate and the asnwer digits are alligned under themselves, so the model can cheat.\n",
        "\n",
        "- Example walkthrough:\n",
        "```\n",
        "x = [4, 7, 10, 6, 11, 3, 5]      \n",
        "y = [-1, -1, -1, -1, 3, 5, -1] # masked + shift\n",
        "```\n",
        "\n",
        "Model receives and creates nn.Embedding\n",
        "```\n",
        "[4, 7, 10, 6, 11, 3, 5]  \n",
        "```\n",
        "\n",
        "Forward through Transformer:\n",
        "```\n",
        "multiple blocks of M-head + FeedForward\n",
        "attend to previous tokens\n",
        "rotary pos embeddings\n",
        "```\n",
        "\n",
        "Compute logits\n",
        "```\n",
        "B, T, vocab_size\n",
        "```\n",
        "\n",
        "Compute loss ignoring the -1\n",
        "```\n",
        "y = [-1, -1, -1, -1, 3, 5, -1]\n",
        "```\n",
        "\n",
        "## Core problem that I didnt get:\n",
        "1. sequence x [4, 7, 10, 6, 11, 3, 5] with length 7, the model produces logits for every postion\n",
        "```\n",
        "(B, T, vocab_size)\n",
        "```\n",
        "- Each logit[t] predict the next token after position t (in next token LM)\n",
        "- I am looking at token t and want to know what is likley to come next!\n",
        "\n",
        "### Examples\n",
        "\n",
        "| Position | Input x | Target y | Masked?                               |\n",
        "| -------- | ------- | -------- | ------------------------------------- |\n",
        "| 0        | 4       | -1       |  ignore input digit                  |\n",
        "| 1        | 7       | -1       | ignore input digit                  |\n",
        "| 2        | '+'     | -1       |  ignore symbol                       |\n",
        "| 3        | 6       | -1       |  ignore input digit                  |\n",
        "| 4        | '='     | 3        |  want to predict first answer digit  |\n",
        "| 5        | 3       | 5        |  want to predict second answer digit |\n",
        "| 6        | 5       | -1       | pad or ignore                       |\n",
        "\n",
        "\n",
        "- Think about this:\n",
        "1. The cast sat on the mat, if you include the word mat in the input the model dosent learn anything it just tries to optimize for the same wording.\n",
        "2. removing the word mat and making it the target will adjust the model graph via backprop so that the tensor of The cast sat on the ... will output mat."
      ],
      "metadata": {
        "id": "pb9wwm_RbKb7"
      }
    }
  ]
}