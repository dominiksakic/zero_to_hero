{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvCqjKoJ8uyi+7phITHg6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/zero_to_hero/blob/main/adv_05_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "Train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.\n"
      ],
      "metadata": {
        "id": "NXg2NidaWDY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## research and refresher\n",
        "\n",
        "- Simple example:\n",
        "  - 1 + 9 = 01\n",
        "  - then tokenize it:     1, 10, 9, 11, 0 , 1\n",
        "  - then create targets: -1, -1, -1, -1, 0, 1\n",
        "  - using ignore_index -1 will not calculate the crossentropy for  the index that match -1 in the targets.\n",
        "\n",
        "\n",
        "### Reminder of cross entropy:\n",
        "- [2.0, 1.0, 0.1]\n",
        "- Target 2\n",
        "\n",
        "- exp(2.0) = 7.3898\n",
        "- exp(1.0) = 2.7128\n",
        "- exp(0.1) = 1.105\n",
        "\n",
        "- sum = 10.212\n",
        "\n",
        "- 7.3898/10 = 0.7235\n",
        "- 2.7128/10 = 0.206\n",
        "- 1.105/10 = 0.108\n",
        "\n",
        "- target is 2 -> 0.108\n",
        "- -log(0.108) => 2.2256\n",
        "\n",
        "\n",
        "## Think about what are the targets of the inputs?\n",
        "- For gpt each token and the rest of the block size will be used to predict the next token."
      ],
      "metadata": {
        "id": "H2hjDA_gqHbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itos = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '=']\n",
        "stoi = {ch:i for i, ch in enumerate(itos)}\n",
        "vocab_size = len(itos)\n",
        "\n",
        "def encode(s):\n",
        "  return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(tokens):\n",
        "  return ''.join([itos[t] for t in tokens])\n",
        "\n",
        "print(encode(\"1+9=01\"))  # [1, 10, 9, 11, 0, 1]\n",
        "print(decode([1, 10, 9, 11, 0, 1]))  # '1+9=01'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP0fGr8xviPG",
        "outputId": "222dd025-dee7-4b98-d77b-ceb8345addfd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10, 9, 11, 0, 1]\n",
            "1+9=01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def make_problem(max_digits: int = 2, reverse_answer: bool = True):\n",
        "    a = random.randint(0, 10**max_digits - 1)\n",
        "    b = random.randint(0, 10**max_digits - 1)\n",
        "    c = a + b\n",
        "    ans = str(c)[::-1] if reverse_answer else str(c)\n",
        "\n",
        "    x = encode(f\"{a}+{b}={ans}\")\n",
        "    y = x[1:] + [-1]\n",
        "\n",
        "    eq_id = stoi['=']\n",
        "    eq_pos = x.index(eq_id)\n",
        "    for t in range(eq_pos):\n",
        "        y[t] = -1\n",
        "\n",
        "    return x, y\n",
        "# test:\n",
        "x, y = make_problem()\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"decoded x:\", decode(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ1rYv-VL24Q",
        "outputId": "a40dabb5-79e6-4076-aeb1-5c4e0915b3b8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [4, 7, 10, 6, 11, 3, 5]\n",
            "y: [-1, -1, -1, -1, 3, 5, -1]\n",
            "decoded x: 47+6=35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(\n",
        "      batch_size: int,\n",
        "      max_digits : int =2,\n",
        "      reverse_answer : bool =True\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    xs, ys = [], []\n",
        "    for _ in range(batch_size):\n",
        "        x, y = make_problem(max_digits, reverse_answer)\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    max_len = max(len(seq) for seq in xs)\n",
        "    for i in range(batch_size):\n",
        "        pad_len = max_len - len(xs[i])\n",
        "        xs[i] = xs[i] + [0] * pad_len\n",
        "        ys[i] = ys[i] + [-1] * pad_len\n",
        "\n",
        "    xs = torch.tensor(xs)\n",
        "    ys = torch.tensor(ys)\n",
        "    xs, ys = xs.to(device), ys.to(device)\n",
        "    return xs, ys\n",
        "\n",
        "# test:\n",
        "xb, yb = make_batch(4)\n",
        "print(xb)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZfvZbSENpQT",
        "outputId": "307e7638-dbeb-4627-9745-effeeb4100ba"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9,  3, 10,  5,  5, 11,  8,  4,  1],\n",
            "        [ 6,  2, 10,  4,  0, 11,  2,  0,  1],\n",
            "        [ 3, 10,  4,  1, 11,  4,  4,  0,  0],\n",
            "        [ 9,  3, 10,  3,  2, 11,  5,  2,  1]], device='cuda:0')\n",
            "tensor([[-1, -1, -1, -1, -1,  8,  4,  1, -1],\n",
            "        [-1, -1, -1, -1, -1,  2,  0,  1, -1],\n",
            "        [-1, -1, -1, -1,  4,  4, -1, -1, -1],\n",
            "        [-1, -1, -1, -1, -1,  5,  2,  1, -1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "max_digits = 2\n",
        "batch_size = 128\n",
        "block_size = 3 * max_digits + 3\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "cEHwKbKqUBif"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLAzSj-UFeN",
        "outputId": "82e8fae9-1feb-48fa-f97d-3a020d179f85"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d3da8f6fdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rope(q, k):\n",
        "    # q, k: (B, n_head, T, head_size), where head_size must be even\n",
        "    B, nh, T, hs = q.shape\n",
        "    assert hs % 2 == 0, \"head_size must be even for RoPE\"\n",
        "    half = hs // 2\n",
        "\n",
        "    freqs = torch.exp(-torch.arange(0, half, dtype=torch.float32) * math.log(10000) / half).to(q.device)  # (half,)\n",
        "    positions = torch.arange(T, device=q.device).float()  # (T,)\n",
        "    angles = torch.einsum('t,d->td', positions, freqs)  # (T, half)\n",
        "    sin = angles.sin().unsqueeze(0).unsqueeze(0)  # (1, 1, T, half)\n",
        "    cos = angles.cos().unsqueeze(0).unsqueeze(0)  # (1, 1, T, half)\n",
        "\n",
        "    q1, q2 = q[..., :half], q[..., half:]\n",
        "    k1, k2 = k[..., :half], k[..., half:]\n",
        "    q_rotated = torch.cat([q1 * cos - q2 * sin, q1 * sin + q2 * cos], dim=-1)\n",
        "    k_rotated = torch.cat([k1 * cos - k2 * sin, k1 * sin + k2 * cos], dim=-1)\n",
        "    return q_rotated, k_rotated"
      ],
      "metadata": {
        "id": "9JovbhvNNukl"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for _ in range(eval_iters):\n",
        "        X, Y = make_batch(batch_size, max_digits=2, reverse_answer=True)\n",
        "        _, l = model(X, Y)\n",
        "        losses.append(l.item())\n",
        "    model.train()\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "3W5uLu3nUHC9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        assert n_embd % num_heads == 0\n",
        "        self.n_head = num_heads\n",
        "        self.head_size = n_embd // num_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Linear projections for q, k, v (combined head projection)\n",
        "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "        # Final projection layer\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # (batch, time, channels)\n",
        "\n",
        "        # Project and reshape into multiple heads\n",
        "        k = self.key(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, self.head_size).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        # Apply Rotary Positional Embedding if available\n",
        "        q, k = apply_rope(q, k)\n",
        "\n",
        "        # Compute attention weights\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, nh, T, T)\n",
        "\n",
        "        # Mask to prevent attending to future tokens (causal attention)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1, 1, T, T)\n",
        "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = wei @ v  # (B, nh, T, hs)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # reassemble heads (B, T, C)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "-FUXyH0JUq3z"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_embd, n_head, dropout)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "FTqIhBDvU1CW"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModelRoPE(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = self.blocks(tok_emb) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets, ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModelRoPE()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgZeKTTOVSC5",
        "outputId": "62683b3e-e825-4c10-e002-faeccc8e8c0a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.794892 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        loss = estimate_loss()\n",
        "        print(f\"step {iter}: random generated validation loss {loss:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = make_batch(batch_size=batch_size, max_digits=max_digits)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQdiY7OjVTP5",
        "outputId": "323a23bc-1db3-470e-ca4e-869ccedd1557"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: random generated validation loss 2.4931\n",
            "step 100: random generated validation loss 1.8513\n",
            "step 200: random generated validation loss 1.8465\n",
            "step 300: random generated validation loss 1.7444\n",
            "step 400: random generated validation loss 0.5344\n",
            "step 500: random generated validation loss 0.0795\n",
            "step 600: random generated validation loss 0.0073\n",
            "step 700: random generated validation loss 0.0035\n",
            "step 800: random generated validation loss 0.0023\n",
            "step 900: random generated validation loss 0.0016\n",
            "step 1000: random generated validation loss 0.0012\n",
            "step 1100: random generated validation loss 0.0009\n",
            "step 1200: random generated validation loss 0.0007\n",
            "step 1300: random generated validation loss 0.0006\n",
            "step 1400: random generated validation loss 0.0005\n",
            "step 1500: random generated validation loss 0.0004\n",
            "step 1600: random generated validation loss 0.0003\n",
            "step 1700: random generated validation loss 0.0003\n",
            "step 1800: random generated validation loss 0.0002\n",
            "step 1900: random generated validation loss 0.0002\n",
            "step 2000: random generated validation loss 0.0002\n",
            "step 2100: random generated validation loss 0.0002\n",
            "step 2200: random generated validation loss 0.0001\n",
            "step 2300: random generated validation loss 0.0001\n",
            "step 2400: random generated validation loss 0.0001\n",
            "step 2500: random generated validation loss 0.0001\n",
            "step 2600: random generated validation loss 0.0001\n",
            "step 2700: random generated validation loss 0.0001\n",
            "step 2800: random generated validation loss 0.0001\n",
            "step 2900: random generated validation loss 0.0001\n",
            "step 3000: random generated validation loss 0.0001\n",
            "step 3100: random generated validation loss 0.0001\n",
            "step 3200: random generated validation loss 0.0001\n",
            "step 3300: random generated validation loss 0.0001\n",
            "step 3400: random generated validation loss 0.0000\n",
            "step 3500: random generated validation loss 0.0000\n",
            "step 3600: random generated validation loss 0.0000\n",
            "step 3700: random generated validation loss 0.0000\n",
            "step 3800: random generated validation loss 0.0000\n",
            "step 3900: random generated validation loss 0.0000\n",
            "step 4000: random generated validation loss 0.0000\n",
            "step 4100: random generated validation loss 0.0000\n",
            "step 4200: random generated validation loss 0.0000\n",
            "step 4300: random generated validation loss 0.0000\n",
            "step 4400: random generated validation loss 0.0000\n",
            "step 4500: random generated validation loss 0.0000\n",
            "step 4600: random generated validation loss 0.0000\n",
            "step 4700: random generated validation loss 0.0000\n",
            "step 4800: random generated validation loss 0.0000\n",
            "step 4900: random generated validation loss 0.0000\n",
            "step 4999: random generated validation loss 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "inp = torch.tensor([encode(\"9+9=\")], dtype=torch.long, device=device)\n",
        "out = model.generate(inp, max_new_tokens=2)\n",
        "print(\"decoded:\", decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLoq4hwGVV-j",
        "outputId": "29db801a-68cc-4ae8-d839-ccb37317cee9"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoded: 9+9=81\n"
          ]
        }
      ]
    }
  ]
}