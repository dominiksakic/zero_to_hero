{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtiWgRL/R6eY7nM2lA3AQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominiksakic/zero_to_hero/blob/main/exercise_batchNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quest\n",
        "E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n"
      ],
      "metadata": {
        "id": "5BiENhx-vZpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93y6f3xF7u4J",
        "outputId": "7a005407-8dfe-46a1-ed8c-6dfc818194ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-25 11:52:56--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-25 11:52:56 (5.75 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "PitX9D9j7xKn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "8hrl29pU7zZd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocab and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i : s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)"
      ],
      "metadata": {
        "id": "GUYlWRWr70IS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build the dataset\n",
        "block_size = 3\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "  for w in words:\n",
        "\n",
        "    #print(w)\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVxCkquX73mA",
        "outputId": "f8b2391c-2eb6-427c-9975-93b92ef4e45c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10\n",
        "n_hidden = 200\n",
        "\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "ZpC9ILuP75eb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_param = sum(p.nelement() for p in parameters)\n",
        "print(f\"Total params: {total_param}\")\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA9IoaqW78Od",
        "outputId": "0245b1b0-17f0-48f6-fefc-b77a57bacf66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 11897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact  = embcat @ W1 + b1\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Yb)\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats, lri for finding a good lr\n",
        "  if i % 10000 == 0:\n",
        "    print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w22Q6b2p8STK",
        "outputId": "78a2e291-0b28-4c43-dac4-db2749a791da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3179\n",
            "  10000/ 200000: 2.1910\n",
            "  20000/ 200000: 2.3270\n",
            "  30000/ 200000: 2.5396\n",
            "  40000/ 200000: 1.9468\n",
            "  50000/ 200000: 2.3331\n",
            "  60000/ 200000: 2.3852\n",
            "  70000/ 200000: 2.1173\n",
            "  80000/ 200000: 2.3159\n",
            "  90000/ 200000: 2.2010\n",
            " 100000/ 200000: 1.8591\n",
            " 110000/ 200000: 2.0881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "rucwuMCo8UT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def calculate_loss(X, Y):\n",
        "  emb = C[X]\n",
        "  embcat =  emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Y)\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "uLPcWQ-d8WdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loss\n",
        "train_loss = calculate_loss(Xtr, Ytr)\n",
        "print(f\"Training loss: {train_loss}\")\n",
        "# validation loss\n",
        "dev_loss = calculate_loss(Xdev, Ydev)\n",
        "print(f\"Dev loss: {dev_loss}\")"
      ],
      "metadata": {
        "id": "5MCMURav8Yhm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}